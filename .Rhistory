full_tab
full_tab = describe_posterior(bmod_full,
estimate = "median", dispersion = T,
ci = .9, ci_method = "hdi",
bf_prior = bmod_full, diagnostic = "Rhat",
test = c("p_direction", "bf"))
full_tab = filter(full_tab, Parameter %notin% c("b_Intercept", "prior_beta", "prior_sigma"))
# -------------------------------------- FREQUENTIST STATS: -----------------------------------------------
# model = aov_car(liking ~ cond + grp + hungry + Error(id/cond), data= PAV.means, factorize = F, anova_table = list(correction = "GG", es = "pes"), type ="II")
# res = nice(model, MSE=F); ref_grid(model) #triple check everything is centered at 0
#
# #calculate Partial eta-squared and its 90 % CI for each effect
# pes_CI = pes_ci(liking ~ cond*grp + age + gender + thirsty + hungry + Error(id/cond), PAV.means, type ="II");
# -------------------------------------- Regression table summary --------------------------------------
tab_model(fmod_full, show.p = F, show.intercept = F, show.se = T, title ="", show.re.var = F, digits = 3, dv.labels = "Perceived liking", file = "tmp/temp2.html", rm.terms = "beta")
report = capture.output(sexit(bmod_full, ci=.9))
full_tab
mcmc_plot(object = bmod_full, pars =c("b_.*o", "b_.*y", "b_.*ge", "b_.*diff", "b_.*V1"), type ="areas")
niter
niter = 5000
warm =1000
# Model
mf = formula(liking ~ condition*intervention*session + age + gender + BMI_V1  + hungry + GLP_diff + reelin_diff + (condition*session|id))
#mf = formula(liking ~ condition*group +  condition*age +  condition*gender + condition*thirsty + condition*hungry  + (1|id))
# -------------------------------------- Bayesian Regression Models using Stan -------------------------------------
bmod_full = brm(mf, data=PAV.means, family = gaussian, prior = c(prior(normal(0, 3), class = "b", coef = ""), prior(normal(0, 100), class = "Intercept", coef = "")), sample_prior = T, save_pars = save_pars(all = TRUE), chains = 4,  iter = 5000, warmup = 1000, seed = 123, inits = 1, backend = 'cmdstanr', threads = threading(4))  # a lot to unwind here..  1) Generic weakly informative prior around 0 for fixed effects and very weak prior for the intercept 2) we need to sample priors and save parameters for computing BF
#lmer to compare
fmod_full <- lmer(update(mf, ~ .-(condition*session|id)+(condition+session|id)) , data=PAV.means) #cannot maximize random structure here
full_tab = describe_posterior(bmod_full,
estimate = "median", dispersion = T,
ci = .9, ci_method = "hdi",
bf_prior = bmod_full, diagnostic = "Rhat",
test = c("p_direction", "bf"))
full_tab = filter(full_tab, Parameter %notin% c("b_Intercept", "prior_beta", "prior_sigma"))
full_tab
niter = 1000
warm = 500
mf = formula(liking ~ condition*intervention*session + age + gender + BMI_V1  + hungry + GLP_diff + reelin_diff + (condition*session+hungry|id))
# -------------------------------------- Bayesian Regression Models using Stan -------------------------------------
bmod_full = brm(mf, data=PAV.means, family = gaussian, prior = c(prior(normal(0, 3), class = "b", coef = ""), prior(normal(0, 100), class = "Intercept", coef = "")), sample_prior = T, save_pars = save_pars(all = TRUE), chains = 4,  iter = 5000, warmup = 1000, seed = 123, inits = 1, backend = 'cmdstanr', threads = threading(4))  # a lot to unwind here..  1) Generic weakly informative prior around 0 for fixed effects and very weak prior for the intercept 2) we need to sample priors and save parameters for computing BF
full_tab = describe_posterior(bmod_full,
estimate = "median", dispersion = T,
ci = .9, ci_method = "hdi",
bf_prior = bmod_full, diagnostic = "Rhat",
test = c("p_direction", "bf"))
full_tab = filter(full_tab, Parameter %notin% c("b_Intercept", "prior_beta", "prior_sigma"))
full_tab
mf
dfdraws = bmod_full %>%
spread_draws(`b_condition`,`b_hungry` )
HDI_cond_PAV = plotHDI( dfdraws$`b_condition` , credMass = .90, binSize = 100, Title = "") + theme_bw()
HDI_cond_PAV
plotHDI( dfdraws$`b_hungry` , credMass = .90, binSize = 100, Title = "") + theme_bw()
full_tab
report[8]
full_tab
report[4]
report[4]; report[10]
mcmc_plot(object = bmod_full, pars =c("b_.*hungry"), type ="areas")
tables <- list.clean(readHTMLTable("tmp/temp3.html"), fun = is.null, recursive = FALSE)
tables2 = tables[[1]] %>% janitor::row_to_names(row_number = 1)
tables2 <- as.matrix(tables2) %>% as_tibble()
tables2
tables2[is.na(tables2)] <- "";  names(tables2) <- NULL; tmp = t(tables2[(length(full_tab$Parameter)+1):(length(full_tab$Parameter)+5),1:2]);
tmp[,2][1] = "R2"; tmp[,2][2] = gsub(".*/","",tmp[,2][2])
pander::pander(tmp[,1:2])
tmp
tables2[is.na(tables2)] <- "";  names(tables2) <- NULL; tmp = t(tables2[(length(full_tab$Parameter)+1):(length(full_tab$Parameter)+4),1:2]);
tmp
tmp[,4][1] = "R2"; tmp[,4][2] = gsub(".*/","",tmp[,2][2])
tmp
pander::pander(tmp[,1:2])
plotHDI( dfdraws$`b_hungry` , credMass = .90, binSize = 100, Title = "") + theme_bw()
mcmc_plot(object = bmod_full, pars =c("b_.*hungry"), type ="areas", ci=0.9)
mcmc_plot(object = bmod_full, pars =c("b_.*hungry"), type ="areas", prob = 0.9)
dfdraws2 =  bmod_full %>%
emmeans(~ condition) %>%
gather_emmeans_draws()
pp = dfdraws2 %>%
ggplot(aes(x = as.factor(condition) , y = .value,  fill = as.factor(condition))) +
geom_abline(slope= 0, intercept=0, linetype = "dashed", color = "black") +
#geom_point(position = "jitter") +
stat_slab(.width = c(0.50, 0.9), position="dodge", alpha=0.5) +
stat_pointinterval(.width = c(0.50, 0.9),position="dodge") +
ylab('Perceived liking')+
xlab('')+
scale_fill_manual(values=c("-1" = pal[1],"1"=pal[2]), guide="none") +
scale_color_manual( values=c("-1" = pal[1],"1"=pal[2]), guide="none") +
scale_x_discrete(labels=c("CS-", "CS+")) +
scale_y_continuous(expand = c(0, 0), breaks = c(seq.int(0,100, by = 25)), limits = c(-0.05,100.5)) +
theme_bw()# + facet_wrap(~group, labeller=labeller(group =labels))
plt_bayes_html = pp + html_theme
plt_bayes = pp + averaged_theme
plt_bayes
dfL <- summarySEwithin(PAV.means,
measurevar = "liking",
withinvars = c("condition", "session"),
idvar = "id")
dfL$cond <- ifelse(dfL$condition == "1", -0.25, 0.25)
pp <- ggplot(PAV.means, aes(x = cond, y = liking,
fill = condition, color = condition)) +
geom_hline(yintercept=50, linetype="dashed", size=0.4, alpha=0.8) +
geom_line(aes(x = condjit, group = id, y = liking), alpha = .3, size = 0.5, color = 'gray' ) +
geom_flat_violin(scale = "count", trim = FALSE, alpha = .2, aes(fill = condition, color = NA)) +
geom_point(aes(x = condjit, shape = intervention), alpha = .3) +
geom_crossbar(data = dfL, aes(y = liking, ymin=liking-se, ymax=liking+se), width = 0.2 , alpha = 0.1)+
ylab('Liking Ratings')+
xlab('Conditioned stimulus')+
scale_y_continuous(expand = c(0, 0), breaks = c(seq.int(0,100, by = 25)), limits = c(-0.05,100.5)) +
scale_x_continuous(labels=c("CS+", "CS-"),breaks = c(-.25,.25), limits = c(-.5,.5)) +
scale_fill_manual(values=c("1"= pal[2], "-1"=  pal[1]), guide = 'none') +
scale_color_manual(values=c("1"= pal[2], "-1"=  pal[1]), guide = 'none') +
scale_shape_manual(name="Intervention", labels=c("Placebo", "Liraglutide"), values = c(1, 2)) +
theme_bw()+ facet_wrap(~session, labeller=labeller(session = labels))
plt = pp + averaged_theme  + theme(legend.position=c(.96,.94), axis.text.x = element_text(size = 14))
pp + html_theme  + theme(legend.position=c(.96,.94), axis.text.x = element_text(size = 14))
PAV.means$cond <- ifelse(PAV.means$condition == "1", -0.25, 0.25)
PAV.means <- PAV.means %>% mutate(condjit = jitter(as.numeric(cond), 0.3),
grouping = interaction(id, cond))
# Liking
dfL <- summarySEwithin(PAV.means,
measurevar = "liking",
withinvars = c("condition", "session"),
idvar = "id")
dfL$cond <- ifelse(dfL$condition == "1", -0.25, 0.25)
pp <- ggplot(PAV.means, aes(x = cond, y = liking,
fill = condition, color = condition)) +
geom_hline(yintercept=50, linetype="dashed", size=0.4, alpha=0.8) +
geom_line(aes(x = condjit, group = id, y = liking), alpha = .3, size = 0.5, color = 'gray' ) +
geom_flat_violin(scale = "count", trim = FALSE, alpha = .2, aes(fill = condition, color = NA)) +
geom_point(aes(x = condjit, shape = intervention), alpha = .3) +
geom_crossbar(data = dfL, aes(y = liking, ymin=liking-se, ymax=liking+se), width = 0.2 , alpha = 0.1)+
ylab('Liking Ratings')+
xlab('Conditioned stimulus')+
scale_y_continuous(expand = c(0, 0), breaks = c(seq.int(0,100, by = 25)), limits = c(-0.05,100.5)) +
scale_x_continuous(labels=c("CS+", "CS-"),breaks = c(-.25,.25), limits = c(-.5,.5)) +
scale_fill_manual(values=c("1"= pal[2], "-1"=  pal[1]), guide = 'none') +
scale_color_manual(values=c("1"= pal[2], "-1"=  pal[1]), guide = 'none') +
scale_shape_manual(name="Intervention", labels=c("Placebo", "Liraglutide"), values = c(1, 2)) +
theme_bw()+ facet_wrap(~session, labeller=labeller(session = labels))
plt = pp + averaged_theme  + theme(legend.position=c(.96,.94), axis.text.x = element_text(size = 14))
pp + html_theme  + theme(legend.position=c(.96,.94), axis.text.x = element_text(size = 14))
pp <- ggplot(PAV.means, aes(x = cond, y = liking,
fill = as.factor(condition), color = as.factor(condition))) +
geom_hline(yintercept=50, linetype="dashed", size=0.4, alpha=0.8) +
geom_line(aes(x = condjit, group = id, y = liking), alpha = .3, size = 0.5, color = 'gray' ) +
geom_flat_violin(scale = "count", trim = FALSE, alpha = .2, aes(fill =  as.factor(condition), color = NA)) +
geom_point(aes(x = condjit, shape =  as.factor(intervention)), alpha = .3) +
geom_crossbar(data = dfL, aes(y = liking, ymin=liking-se, ymax=liking+se), width = 0.2 , alpha = 0.1)+
ylab('Liking Ratings')+
xlab('Conditioned stimulus')+
scale_y_continuous(expand = c(0, 0), breaks = c(seq.int(0,100, by = 25)), limits = c(-0.05,100.5)) +
scale_x_continuous(labels=c("CS+", "CS-"),breaks = c(-.25,.25), limits = c(-.5,.5)) +
scale_fill_manual(values=c("1"= pal[2], "-1"=  pal[1]), guide = 'none') +
scale_color_manual(values=c("1"= pal[2], "-1"=  pal[1]), guide = 'none') +
scale_shape_manual(name="Intervention", labels=c("Placebo", "Liraglutide"), values = c(1, 2)) +
theme_bw()+ facet_wrap(~session, labeller=labeller(session = labels))
plt = pp + averaged_theme  + theme(legend.position=c(.96,.94), axis.text.x = element_text(size = 14))
pp + html_theme  + theme(legend.position=c(.96,.94), axis.text.x = element_text(size = 14))
mf1 = formula(grips ~ trial*intervention*session + hungry + (session|id) + (1|trial)) # LINEAR FIT
mf2 = formula(grips ~  lspline(trial,5)*interventionsession + hungry + (session|id) + (1|trial)) # PIECEWISE REGRESSION WITH SPLINE AT 5
# -------------------------------------- Bayesian Regression Models using Stan -------------------------------------
linmod = brm(mf1, data=INST, family = gaussian, prior = c(prior(normal(0, 3), class = "b", coef = ""), prior(normal(0, 100), class = "Intercept", coef = "")), sample_prior = T, save_pars = save_pars(all = TRUE), chains = chains,  iter = niter, warmup = warm, seed = 123, inits = 1, backend = 'cmdstanr', threads = threading(4))
splinemod = update(linmod, formula. = mf2)
mf2
linmod
splinemod = update(linmod, formula. = mf2)
mf2 = formula(grips ~  lspline(trial,5)*intervention*session + hungry + (session|id) + (1|trial)) # PIECEWISE REGRESSION WITH SPLINE AT 5
INST$trial
INST$trial = as.numeric(as.character(INST$trial))
# Model
mf1 = formula(grips ~ trial*intervention*session + hungry + (session|id) + (1|trial)) # LINEAR FIT
mf2 = formula(grips ~  lspline(trial,5)*intervention*session + hungry + (session|id) + (1|trial)) # PIECEWISE REGRESSION WITH SPLINE AT 5
# -------------------------------------- Bayesian Regression Models using Stan -------------------------------------
linmod = brm(mf1, data=INST, family = gaussian, prior = c(prior(normal(0, 3), class = "b", coef = ""), prior(normal(0, 100), class = "Intercept", coef = "")), sample_prior = T, save_pars = save_pars(all = TRUE), chains = chains,  iter = niter, warmup = warm, seed = 123, inits = 1, backend = 'cmdstanr', threads = threading(4))  # a lot to unwind here..  1) Generic weakly informative prior around 0 for fixed effects and very weak prior for the intercept 2) we need to sample priors and save parameters for computing BF
splinemod = update(linmod, formula. = mf2)
splinemod
linmod$loo = loo(linmod); splinemod$loo = loo(splinemod)
comp = loo::loo_compare(linmod$loo, splinemod$loo)
comp
bmod_full = splinemod #winner model
#lmer to compare
fmod_full = lmer(mf2 , data = INST, REML=F, control = control )
onditional_effects(bmod_full,effects = "trial")
conditional_effects(bmod_full,effects = "trial")
conditional_effects(bmod_full,effects = "trial:session")
INST$session = INST$session = ifelse(INST$session == "0", -1, 1)
bmod_full
INST$intervention
INST$intervention = ifelse(INST$intervention == "0", -1, 1)  #change value of
# Model
mf1 = formula(grips ~ trial*intervention*session + hungry + (session|id) + (1|trial)) # LINEAR FIT
mf2 = formula(grips ~  lspline(trial,5)*intervention*session + hungry + (session|id) + (1|trial)) # PIECEWISE REGRESSION WITH SPLINE AT 5
# -------------------------------------- Bayesian Regression Models using Stan -------------------------------------
linmod = brm(mf1, data=INST, family = gaussian, prior = c(prior(normal(0, 3), class = "b", coef = ""), prior(normal(0, 100), class = "Intercept", coef = "")), sample_prior = T, save_pars = save_pars(all = TRUE), chains = chains,  iter = niter, warmup = warm, seed = 123, inits = 1, backend = 'cmdstanr', threads = threading(4))  # a lot to unwind here..  1) Generic weakly informative prior around 0 for fixed effects and very weak prior for the intercept 2) we need to sample priors and save parameters for computing BF
splinemod = update(linmod, formula. = mf2)
linmod$loo = loo(linmod); splinemod$loo = loo(splinemod)
comp = loo::loo_compare(linmod$loo, splinemod$loo)
bmod_full = splinemod #winner model
#lmer to compare
fmod_full = lmer(mf2 , data = INST, REML=F, control = control )
library(repro)
# load packages from yaml header
automate_load_packages()
# include external scripts
automate_load_scripts()
# load data
intern  <- automate_load_data(intern, read.csv, stringsAsFactors = T)
medic    <- automate_load_data(medic, read.csv, stringsAsFactors = T)
PAV      <- automate_load_data(PAV, read.csv, stringsAsFactors = T)
INST     <- automate_load_data(INST, read.csv, stringsAsFactors = T)
PIT      <- automate_load_data(PIT, read.csv, stringsAsFactors = T)
HED      <- automate_load_data(HED, read.csv, stringsAsFactors = T)
HED_fMRI <- automate_load_data(HED_fMRI, read.csv, stringsAsFactors = T)
x = session_info();  opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE) # set F for all
## we recommend running this is a fresh R session or restarting your current session
#install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
#install_cmdstan()
# check_git(); check_make(); check_docker() #check if installed
sessio = session_info(); #opts_chunk$set(echo = F, message=F, warning=F) # set echo F for all
niter = 500; warm = 100; chains = 4; cores = 4; nsim = 10000 # number of iterations (to change if you want to quick check and warmups #or also parallel::detectCores()/2)
options(scipen = 666, warn=-1, contrasts=c("contr.sum","contr.poly"), mc.cores = cores);  #remove scientific notation # remove warnings #set contrasts to sum !
#cl = parallel::detectCores()/2
set.seed(666) #set random seed
control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')) #set "better" lmer optimizer #nolimit # yoloptimizer
#emm_options(pbkrtest.limit = 8000) #increase repetitions limit for frequentist stats
source('R/plots.R', echo=F)# plot specification
source('R/utils.R', echo=F)# useful functions
panderOptions('knitr.auto.asis', FALSE) #remove auto styling
# Look at R/clean.R (listed in the YAML) which does all the preprocessing for more info
# If you are unsure weather or not you have `git` `make` & `docker`.
# check_git()
# check_make()
# check_docker()
#subset only obese
tables <- c("PAV","INST","PIT","HED", "intern")
dflist <- lapply(mget(tables),function(x)subset(x, group == 'obese'))
list2env(dflist, envir=.GlobalEnv)
#exclude participants (242 really outlier everywhere, 256 can't do the task, 228 REALLY hated the solution and thus didn't "do" the conditioning)
# dflist <- lapply(mget(tables),function(x)filter(x, id %notin% c(242, 256, 228))
# list2env(dflist, envir=.GlobalEnv)
#center covariates paste0(names(PAV), collapse="','")
medic$ageF = medic$age; medic$weightLoss = -(medic$BMI_diff); medic$bmi1 = medic$BMI_V1; medic$dif_interv = medic$Date_diff #keep uncentered for descriptive stats + reverse BMI_diff so it in terms of actual weight loss and not weight gain
biomed <- c('age','Date_diff','BW_diff','BMI_diff','WC_diff','Insulin_diff','X2.AG_diff','reelin_diff','MCP_diff','TNFalpha_diff','GLP_diff','Ghrelin_diff','Glu_diff','HOMA_IR_diff', 'AEA_diff', 'OEA_diff', 'PEA_diff', 'BMI_V1')
medic = medic %>% group_by %>% mutate_at(biomed, scale)
#remove outliers from biomedical (+- 3 SD)
df_dict <- data.frame(variable = biomed, out_low = rep(-3,length(biomed)),  out_high = rep(3,length(biomed)))
for (var in df_dict$variable) {
medic[[var]][medic[[var]] < df_dict[df_dict$variable == var, ]$out_low | medic[[var]] > df_dict[df_dict$variable == var, ]$out_high] <- NaN}
#merge with medic
tables = tables[-length(tables)]; # remove intern
dflist <- lapply(mget(tables),function(x)merge(x, medic, by = "id"))
list2env(dflist, envir=.GlobalEnv)
# creates internal states variables for each data
listA = 2:5
def = function(data, number){
baseINTERN = subset(intern, phase == number)
data = merge(x = get(data), y = baseINTERN[ , c("thirsty", 'hungry',  'piss', 'id', 'session')], by = c("id", 'session'), all.x=TRUE)
return(data)
}
dflist = mapply(def,tables,listA)
list2env(dflist, envir=.GlobalEnv)
#center covariates
numer <- c('thirsty','hungry')
dflist <- lapply(mget(tables),function(x) x %>% group_by %>% mutate_at(numer, scale))
list2env(dflist, envir=.GlobalEnv)
covariate = c(biomed, numer)
# prepro RT PAV -----------------------------------------------------------
# get times in milliseconds
PAV$RT    <- PAV$RT * 1000
#Preprocessing
PAV$condition <- droplevels(PAV$condition, exclude = "Baseline")
acc_bef = mean(PAV$ACC, na.rm = TRUE) # 0.87
full = length(PAV$RT)
##shorter than 100ms and longer than 3sd+mean
PAV.clean <- filter(PAV, RT >= 100) # min RT is
PAV.clean <- ddply(PAV.clean, .(id, session), transform, RTm = mean(RT), RTsd = sd(RT))
PAV.clean <- filter(PAV.clean, RT <= RTm+3*RTsd)
# calculate the dropped data in the preprocessing
clean = length(PAV.clean$RT)
dropped = full-clean
(dropped*100)/full #13.26754
# clean PAV --------------------------------------------------------------
PAV = PAV.clean
# define as.factors
fac <- c("id", "trial", "condition", "session", "intervention","trialxcondition", "gender"); PAV[fac] <- lapply(PAV[fac], factor)
#revalue all catego
PAV$session = PAV$session = ifelse(PAV$session == "second", -1, 1)
#as.factor(revalue(PAV$session, c(second="-1", third="1"))) #change value of session
PAV$condition = ifelse(PAV$condition == "CSminus", -1, 1)
PAV$intervention = ifelse(PAV$intervention == "0", -1, 1)  #change value of intervention
#PAV$condition = as.factor(revalue(PAV$condition, c(CSminus="-1", CSplus="1")));
PAV.means <- aggregate(PAV[,c(covariate, "weightLoss", "ageF",  "bmi1", "liking", "RT")] , by = list(PAV$id, PAV$condition,PAV$session,PAV$intervention, PAV$gender), FUN = 'mean',na.action = na.omit)
colnames(PAV.means) <- c('id','condition','session','intervention', 'gender', covariate,"weightLoss", "ageF", "bmi1", "liking", "RT")
#imput mean (0) for the two covariate (MAR) so we can get BF (missing values fot 2 participant 262 and 232)
PAV.means$thirsty[is.na(PAV.means$thirsty)] <- 0 ; PAV.means$hungry[is.na(PAV.means$hungry)] <- 0
# clean INST --------------------------------------------------------------
INST$Trial = as.numeric(INST$trial)
x = lspline(INST$Trial, 5); INST$Trial1 = x[,1]; INST$Trial2 = x[,2];
# define as.factors
fac <- c("id", "session", "intervention", "gender"); INST[fac] <- lapply(INST[fac], factor)
#revalue all catego
INST$session = INST$session = ifelse(INST$session == "second", -1, 1)
#as.factor(revalue(PAV$session, c(second="-1", third="1"))) #change value of session
INST$intervention = ifelse(INST$intervention == "0", -1, 1)  #change value of intervention
# INST$session = as.factor(revalue(INST$session, c(second="0", third="1"))) #change value of session
INST.means <- aggregate(INST[,c(covariate, "grips")] , by = list(INST$trial, INST$session,INST$intervention, INST$gender), FUN = 'mean',na.action = na.omit)
colnames(INST.means) <- c('trial','session','intervention', 'gender', covariate, "grips")
#imput mean (0) for the two covariate (MAR) so we can get BF (missing values for 3 participant 239, 258, 231)
INST.means$thirsty[is.na(INST.means$thirsty)] <- 0 ; INST.means$hungry[is.na(INST.means$hungry)] <- 0
INST.means$Trial = as.numeric(INST.means$trial)
x = lspline(INST.means$Trial, 5); INST.means$Trial1 = x[,1]; INST.means$Trial2 = x[,2];
dfTrial = ddply(INST,.(trial,session),summarise,grips=mean(grips)); dfTrial$Trial = scale(as.numeric(dfTrial$trial))
dfTrial$phasis = ifelse(dfTrial$Trial >	-1.07212710 , "1", "0")
dfTrial$T2 = ifelse(dfTrial$Trial > 0, dfTrial$Trial^2, -dfTrial$Trial^2)
# clean PIT --------------------------------------------------------------
PIT = subset(PIT, condition != "BL")
# define as.factors
fac <- c("id", "trial", "condition", "session", "intervention","trialxcondition", "gender"); PIT[fac] <- lapply(PIT[fac], factor)
#revalue all catego
PIT$session = as.factor(revalue(PIT$session, c(second="0", third="1"))) #change value of session
PIT$condition = as.factor(revalue(PIT$condition, c(CSminus="-1", CSplus="1"))); #PIT$condition <- factor(PIT$condition, levels = c("1", "-1"))#change value of condition
PIT.means <- aggregate(PIT[,c(covariate, "AUC")] , by = list(PIT$id, PIT$condition,PIT$session,PIT$intervention, PIT$gender), FUN = 'mean',na.action = na.omit)
colnames(PIT.means) <- c('id','condition','session','intervention', 'gender', covariate, "AUC")
#imput mean (0) for the two covariate (MAR) so we can get BF (missing values fot 2 participant 229 and 238)
PIT.means$thirsty[is.na(PIT.means$thirsty)] <- 0 ; PIT.means$hungry[is.na(PIT.means$hungry)] <- 0
# clean HED --------------------------------------------------------------
#create and center int covariate
HED$lik = HED$perceived_liking #rename
dfl = ddply(HED,.(id,condition,session),summarise, fam=mean(perceived_familiarity), int=mean(perceived_intensity));
dfi = subset(dfl, condition  =="MilkShake"); dfi$int = dfl$int[dfl$condition  =="MilkShake"] -dfl$int[dfl$condition  =="Empty"];  dfi$fam = dfl$fam[dfl$condition  =="MilkShake"] -dfl$fam[dfl$condition  =="Empty"]; dfi$int = scale(dfi$int); dfi$fam = scale(dfi$fam); dfi = dfi[-c(2)]
HED = merge(HED, dfi, by = c("id", "session"))
# define as.factors
fac <- c("id", "trial", "condition", "session", "intervention","trialxcondition", "gender"); HED[fac] <- lapply(HED[fac], factor)
#revalue all catego
HED$session = as.factor(revalue(HED$session, c(second="0", third="1"))) #change value of session
HED$condition = as.factor(revalue(HED$condition, c(Empty="-1", MilkShake="1")));#HED$condition <- factor(HED$condition, levels = c("1", "-1"))#change value of condition
HED.means <- aggregate(HED[,c(covariate, "fam", "int", "lik")] , by = list(HED$id, HED$condition,HED$session,HED$intervention, HED$gender), FUN = 'mean', na.action = na.omit)
colnames(HED.means) <- c('id','condition','session','intervention', 'gender', covariate, "fam", "int", "lik")
#imput mean (0) for the two covariate (MAR) so we can get BF (missing values fr 1 participant 217)
HED.means$thirsty[is.na(HED.means$thirsty)] <- 0 ; HED.means$hungry[is.na(HED.means$hungry)] <- 0
# ALL ---------------------------------------------------------------------
#factorize ID
tables <- c("PAV.means","PIT.means","HED.means")
dflist <- lapply(mget(tables),function(x)facID(x))
list2env(dflist, envir=.GlobalEnv)
save(PAV.means, file = "data/PAV.Rdata")
save(INST.means, file = "data/INST.Rdata")
save(PIT.means, file = "data/PIT.Rdata")
save(HED.means, file = "data/HED.Rdata")
#create df for AFNI
dfHED = HED.means
dfHED[is.na(dfHED)] <- 0
save(dfHED, file = "data/HED_fmri.Rdata")
# internHED = subset(intern, phase ==5)
# dfx = x = Reduce(function(x,y) merge(x = x, y = y, by = "id"),
#                  list(df, internHED, info)); dfx[is.na(dfx)] <- 0
# x = Reduce(function(x,y) merge(x = x, y = y, by = c("id","session")),
#            list(dfl, dfm, dfx))
#
# tables = c('x')
# numer <- c("thirsty", "hungry",  "piss", "OEA", "PEA","X2.AG","AEA","Leptin",  "Resistin","adiponectin","MCP","TNFalpha","reelin","glucagon", "Ghrelin","obestatin","GLP1","insulin","Fast_glu","BMI_t1", "bmi_diff")
# dflist <- lapply(mget(tables),function(x) x %>% group_by %>% mutate_at(numer, scale))
# list2env(dflist, envir=.GlobalEnv); x$age = x$age_Z
#
# dfHED = select(x, -c(age_Z, bmi1, bmi_dif, task, phase, idXsession, BMI_t2, group))
# dfHED$session = as.factor(revalue(as.factor(dfHED$session), c("second"="0", "third"="1"))); dfHED1 = dfHED;  dfHED2 = dfHED; dfHED1$condition = "1"; dfHED2$condition = "-1"; dfHED = rbind(dfHED1, dfHED2); dfHED[is.na(dfHED)] <- 0
#
# save(dfHED, file = "data/HED_fmri.Rdata")
#
# create df for weight loss
df = subset(PAV.means, session == "1"); df = subset(df, condition == "1")
df$AGE = df$ageF; df$BMI = df$bmi1 ; df$GENDER = df$gender; df$INTERVENTION = df$intervention
df$intervention = ifelse(df$intervention == 0, -1, 1) #change value of intervention
df$gender = ifelse(df$gender == 0, -1, 1) #change value of gender
df$INTERVENTION = as.factor(revalue(as.factor(df$INTERVENTION), c("0"="Placebo", "1"="Liraglutide")))#using pav.means but oculd be any other
df$GENDER = as.factor(revalue(as.factor(df$GENDER), c("0"="Men", "1"="Women")))
med <- gather(df, "feature", "n", 8:22)
#biomed = numer[3:14];
dfmed = na.omit(medic[,c('intervention',biomed[3:14])]) #create df for var selec
#inter score for fmri Plots
diffPRE = subset(HED_fMRI, session == 'pre') ; diffPOST = subset(HED_fMRI, session == 'post')
inter = diffPRE; inter$OFC_inter =  diffPOST$OFC_score - diffPRE$OFC_score; inter$HF_inter =  diffPOST$HF_score - diffPRE$HF_score
inter$id = as.factor(inter$id); inter$intervention = as.factor(inter$intervention)
inter = filter(inter, id %notin% c(246)) #remove huge outlier because it biases the whole further results on mediation and weigthloss -> you can check it ou via:
# ggplot(inter, aes(x= BMI_diff, y=intervention, label=id))+
#   geom_point() +geom_text(aes(label=id),hjust=1, vjust=0)
weightloss = inter$BMI_diff; #reverse in terms of wight loss not weigth gain
ind <- sapply(inter, is.numeric)
inter[ind] <- lapply(inter[ind], scale)
inter$Intervention = as.numeric(inter$intervention); inter$HF  = as.vector(inter$HF_inter); inter$`Weight Loss` = -as.vector(weightloss) #reverse to have it in positive terms
egltable(c("BMI", "AGE", "GENDER"),
g = "INTERVENTION", data = df, strict = FALSE) %>%
kbl(caption ="Summary statistics", digits = 2) %>%
kable_styling(latex_options = "HOLD_position", position = "center", full_width = F) %>%
row_spec(0,bold=T,align='c')
# Model
mf1 = formula(grips ~ trial*intervention*session + hungry + (session|id) + (1|trial)) # LINEAR FIT
mf2 = formula(grips ~  lspline(trial,5)*intervention*session + hungry + (session|id) + (1|trial)) # PIECEWISE REGRESSION WITH SPLINE AT 5
# -------------------------------------- Bayesian Regression Models using Stan -------------------------------------
linmod = brm(mf1, data=INST, family = gaussian, prior = c(prior(normal(0, 3), class = "b", coef = ""), prior(normal(0, 100), class = "Intercept", coef = "")), sample_prior = T, save_pars = save_pars(all = TRUE), chains = chains,  iter = niter, warmup = warm, seed = 123, inits = 1, backend = 'cmdstanr', threads = threading(4))  # a lot to unwind here..  1) Generic weakly informative prior around 0 for fixed effects and very weak prior for the intercept 2) we need to sample priors and save parameters for computing BF
splinemod = update(linmod, formula. = mf2)
linmod$loo = loo(linmod); splinemod$loo = loo(splinemod)
comp = loo::loo_compare(linmod$loo, splinemod$loo)
bmod_full = splinemod #winner model
#lmer to compare
fmod_full = lmer(mf2 , data = INST, REML=F, control = control )
INST$gender
INST$gender = INST$gender = ifelse(INST$gender == "0", -1, 1)
# Model
mf1 = formula(grips ~ trial*intervention*session + age + gender + BMI_V1  + hungry + GLP_diff + reelin_diff + (session|id) + (1|trial)) # LINEAR FIT
mf2 = formula(grips ~  lspline(trial,5)*intervention*session + age + gender + BMI_V1  + hungry + GLP_diff + reelin_diff + (session|id) + (1|trial)) # PIECEWISE REGRESSION WITH SPLINE AT 5
# -------------------------------------- Bayesian Regression Models using Stan -------------------------------------
linmod = brm(mf1, data=INST, family = gaussian, prior = c(prior(normal(0, 3), class = "b", coef = ""), prior(normal(0, 100), class = "Intercept", coef = "")), sample_prior = T, save_pars = save_pars(all = TRUE), chains = chains,  iter = niter, warmup = warm, seed = 123, inits = 1, backend = 'cmdstanr', threads = threading(4))  # a lot to unwind here..  1) Generic weakly informative prior around 0 for fixed effects and very weak prior for the intercept 2) we need to sample priors and save parameters for computing BF
splinemod = update(linmod, formula. = mf2)
linmod$loo = loo(linmod); splinemod$loo = loo(splinemod)
comp = loo::loo_compare(linmod$loo, splinemod$loo)
bmod_full = splinemod #winner model
#lmer to compare
fmod_full = lmer(mf2 , data = INST, REML=F, control = control )
comp = rownames_to_column(as_tibble(comp, rownames = NA)); colnames(comp)[1] = "model"
comp = tibble::(as_tibble(comp, rownames = NA)); colnames(comp)[1] = "model"
comp = tibble::rownames_to_column(as_tibble(comp, rownames = NA)); colnames(comp)[1] = "model"
pander(comp[c(1:3,8:9)]) # so the splinemod was preferred as the other model had smaller ELPD. Thus, spline improved out-of-sample predictions. can also see: compare_ic(linmod, splinemod) but its deprecated
mcmc_plot(object = bmod_full, pars =c("b_.al*","b_.o*", "b_.*y", "b_.*ge", "b_.*diff", "b_.*V1"), type ="areas")
mcmc_plot(object = bmod_full, pars =c("b_.al*"), type ="areas")
mcmc_plot(object = bmod_full, pars =c("b_.*al"), type ="areas")
mcmc_plot(object = bmod_full, pars =c("b_.*al", "b_.*ses"), type ="areas")
mcmc_plot(object = bmod_full, pars =c("b_.*al", "b_.*intervention"), type ="areas")
mcmc_plot(object = bmod_full, pars =c("b_.*al", "b_session"), type ="areas")
mcmc_plot(object = bmod_full, pars =c("b_.*al"), type ="areas")
describe_posterior(bmod_full,
estimate = "median", dispersion = T,
ci = .9, ci_method = "hdi",
bf_prior = bmod_full, diagnostic = "Rhat",
test = c("p_direction", "bf"))
mcmc_plot(object = bmod_full, pars =c("b_.*y", "b_.*ge", "b_.*diff", "b_.*V1"), type ="areas")
pp_check(bmod_full, type = "stat_grouped", group = "intervention", binwidth = 0.01, nsamples = NULL) #equality of variance between groups
pp_check(bmod_full, nsamples = 10)
pp_check(bmod_full, type ="error_scatter_avg", nsamples = NULL)
residuals <-residuals(bmod_full)[, 1]; res <- qqnorm(residuals, pch = 1)
plot_model(fmod_full, type = "diag");
full_tab = describe_posterior(bmod_full,
estimate = "median", dispersion = T,
ci = .9, ci_method = "hdi",
bf_prior = bmod_full, diagnostic = "Rhat",
test = c("p_direction", "bf"))
full_tab = filter(full_tab, Parameter %notin% c("b_Intercept", "prior_beta", "prior_sigma"))
x = attributes(full_tab); x$clean_parameters
x$clean_parameters[c(2:3,6:9,11:12),5]
x$clean_parameters[,5]
x$clean_parameters[1:18,5]
x$clean_parameters[2:3,5]
x$clean_parameters[6:9,5]
x$clean_parameters[12:15,5]
x$clean_parameters[12:18,5]
x$clean_parameters[12:19,5]
x$clean_parameters[2:3,5]
x$clean_parameters[12:18,5]
x$clean_parameters[12:15,5]
x$clean_parameters[16:18,5]
x$clean_parameters[17:18,5]
x = attributes(full_tab); x$clean_parameters[c(2:3,12:15,17:18),5] = c("trial<5",       "trial>5"  , "trial<5:intervention"  , "trial>5:intervention",  "trial<5:session", "trial>5:session", "trial<5:intervention:session" , "trial>5:group:session"); attributes(full_tab) = x
full_tab
tab_model(bmod_full, show.p = F, show.intercept = F, show.se = T, title ="", show.re.var = F, digits = 3, dv.labels = "Number of Grips", file = "tmp/temp4.html", transform = NULL,  rm.terms = "beta")
report = capture.output(sexit(bmod_full, ci=.9))
report::report_model(bmod_full)
full_tab
report[4]
report[5]
report[7]
report[4,5,7]
report[c(4,5,7)]
tables <- list.clean(readHTMLTable("tmp/temp4.html"), fun = is.null, recursive = FALSE)
tables2 = tables[[1]] %>% janitor::row_to_names(row_number = 1)
tables2 <- as.matrix(tables2) %>% as_tibble()
tables2[is.na(tables2)] <- "";  names(tables2) <- NULL; tmp = t( tables2[(length(full_tab$Parameter)+1):(length(full_tab$Parameter)+5),1:2]);
tables2
tmp
tmp[,5][1] = "R2"; tmp[,5][2] = gsub(".*/","",tmp[,4][2])
tmo
tmp
tmp[,5][1] = "R2"; tmp[,5][2] = gsub(".*/","",tmp[,5][2])
tmp
tables2[is.na(tables2)] <- "";  names(tables2) <- NULL; tmp = t( tables2[(length(full_tab$Parameter)+1):(length(full_tab$Parameter)+5),1:2]);
tmp[,5][1] = "R2"; tmp[,5][2] = gsub(".*/","",tmp[,5][2])
pander::pander(tmp)
pwd
setwd("~/Desktop/SwitchDrive/T0")
tables <- list.clean(readHTMLTable("tmp/temp3.html"), fun = is.null, recursive = FALSE)
tables2 = tables[[1]] %>% janitor::row_to_names(row_number = 1)
tables2 <- as.matrix(tables2) %>% as_tibble()
tables2[is.na(tables2)] <- "";  names(tables2) <- NULL; tmp = t( tables2[(length(full_tab$Parameter)+1):(length(full_tab$Parameter)+5),1:2]);
tmp
table
tables
tables2 = tables[[1]] %>% janitor::row_to_names(row_number = 1)
tables2 <- as.matrix(tables2) %>% as_tibble()
tables2
library(repro)
# load packages from yaml header
automate_load_packages()
# include external scripts
automate_load_scripts()
# load data
intern <- automate_load_data(intern, read.csv, stringsAsFactors = T)
PAV <- automate_load_data(PAV, read.csv, stringsAsFactors = T)
INST <- automate_load_data(INST, read.csv, stringsAsFactors = T)
PIT <- automate_load_data(PIT, read.csv, stringsAsFactors = T)
HED <- automate_load_data(HED, read.csv, stringsAsFactors = T)
medic <- automate_load_data(medic, read.csv, stringsAsFactors = T)
pupil <- automate_load_data(pupil, read.csv, stringsAsFactors = T)
